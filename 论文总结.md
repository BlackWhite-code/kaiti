# 论文总结

[toc]

论文发布于2017年5月

## `Prototypical Networks for Few-shot Learning`

解决问题：通过解决过拟合的关键问题来解决小样本的问题。

提出一个假设：存在一个嵌入，其中点围绕每个类的原型表示聚集。在这个前提下，可以使用神经网络来学习输入到嵌入空间的非线性映射，并将类的原型作为嵌入空间中其支持集的平均值。分类就是看离那个原型的距离最近。

此外，还证明了当使用布雷格曼散数（平方欧几里得距离）计算距离时，使用类均值作为原型是合理的。

经验上发现欧几里得距离远远优于更常用的余弦相似度。（后面大部分用的都是点积距离，点积一般代替的是余弦相似度）







---

论文是NAACL2022 7月

## `Learn from Relation Information: Towards Prototype Representation Rectification for Few-Shot Relation Extraction`

目的是为了解决小样本关系抽取，为了更好的适应小样本的新关系类，为了解决大多数方法在隐式引入关系信息（关系信息或关系描述），然后来约束原型表示学习，例如：对比学习，图和专门的注意力机制，这可能会带来无用甚至有害得参数（其中另一篇ACL的论文直接表示可以用直接相加的方式来进行操作），第二个方面，对于弱隐式约束作用，可能这些方法在处理远离类中心的异常值样本时受到限制。通俗一点说，就是原来的原型和用小样本来进行计算的样本原型偏差很大的。

提出了一个PRM机制来显式引入关系信息。主要有两个门结构组成。一个门决定原始原型的剩余量，另一个门用关系信息来更新剩余的原型。这样，就可以解决上面的第二个问题，更好的稳健地处理异常值。

![image-20221110172117288](C:\Users\jerry\AppData\Roaming\Typora\typora-user-images\image-20221110172117288.png)

下面重点介绍一下模型的组成。

![image-20221110174159936](C:\Users\jerry\AppData\Roaming\Typora\typora-user-images\image-20221110174159936.png)

语句编码：本篇论文用的BERT作为编码器，通过连接两个实体提及的开始标记对应的隐藏状态来标记成`Original Prototype`（根据另一篇MTB的研究来说，实体），通俗来说（In more detail）通常通过BERT编码获得支持集中每个关系类中K个实例的语句表示，然后取K个语句表示的平均值以获得关系原型。上图中虚线圆来表示的。（由于原型完全是由支持集中为每个关系类型提供的K个实例中获得的，一旦K个实例不够“好”，并且离真正的类中心太远，就会导致模型做出错误的预测。）

关系编码：我们用模板“name:description”连接名称和描述，然后输入到BERT中进行嵌入。详细来说就是将[CLS]标记对应的隐藏状态与所有标记的隐藏状态的平均连接起来。关系类的名称和描述是可以表示整个类分布的原始信息，并且对于FSRE任务来说非常容易访问。（当然都是论文作者这么认为的，不知道对不对）通俗来说就是关系编码中有全局分布信息，K个实例中含有局部分布信息，然后矫正之后的原型（图中的三角形）既包含全局分布信息，也包含每个关系类给定的K个特定实例的局部分布信息。（与一篇论文中的全局和局部信息拼接有异曲同工之妙）

校正模块：基于GRU的影响，其实也可以是LSTM中的影响，两个门来控制。直接用公式可能会比较清楚一点。

首先，通过对关系和原始原型执行门机制来获得引入多少关系信息以及原始原型应该替换多少关系信息：
$$
r^i= \sigma(W_r·[R^i,P^i_{ori}]+b_r)
$$

$$
R^i_{remain}=([1]-r^i)×R^i
$$

$$
R^i_{replace} = r^i × R^i
$$

$$
\sigma是sigmoid函数，通过这个函数可以将数据变换成0-1范围内的数值，从而来充当门控信号
$$
$$
p^i = \sigma(W_p·[R^i_{replace},P^i_{ori} + b_p])
$$

$$
P^i_{ori-remain} = p^i ×P^i,i =1,...,N
$$

$$
P^i_{rec}=R^i_{remain}+P^i_{ori-remain}
$$



最后模型使用向量点积方法来计算查询实例Q和每个校正原型之间的距离，然后将该距离反馈给交叉熵损失，来形成训练损失，这个类似于对比损失，
$$
L = -\sum^G_{j=1}\log{\frac {exp({P^i_{rec}·Q_j})} {\sum^N_{i=1} exp(P^i_{rec}·Q_j)}}
$$


最后的预测阶段，模型再一次计算校正原型和查询实例之间的距离，然后选择距离最短的关系作为预测结果。

结果：

![image-20221116140520874](C:\Users\jerry\AppData\Roaming\Typora\typora-user-images\image-20221116140520874.png)

---

论文发布在ACL2022年5月

## `A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction`

对于以前的用关系信息（包括关系标签和描述）隐含地约束每个关系类的原型，通常通过设计复杂的网络结构，生成混合特征，结合对比学习或者注意力网络，或者其他的复杂的网络结构，本文提出一个比较简单而且有效的方法来引入模型。这个方法就是直接相加，向量的直接相加可以达到上述复杂的所有的效果。

对于每个关系类来说，首先通过连接两个关系视图（即[CLS]标记嵌入和所有标记嵌入的平均值）来生成关系表示，然后直接添加到原始原型中用于训练和预测。

以往方法的问题：第一，大多都采用了隐式约束，如对比学习和关系图，不是直接融合，而面对远程样本时可能很弱。第二，它们通常采用负责的设计或者网络，如混合特征或精心设计的注意力网络，这样可能会带来太多甚至有害的参数。

采用的方法：简单有效，直接相加，相同的编码器用于关系信息和句子进行编码，以将它们映射到相同的语义空间里，另一方面，通过连接两个关系视图（[CLS]标记嵌入和所有标记嵌入的平均值）为每个关系类生成关系表示 ，这允许关系表示和原型形成相同的维度。然后，将生成的关系表示直接添加到原型中，以增强模型训练和预测。

![image-20221111163308167](C:\Users\jerry\AppData\Roaming\Typora\typora-user-images\image-20221111163308167.png)

两个原因：第一，当面对远程样本时，直接相加比隐式约束更能生成有前景的原型。第二，直接添加不会加入额外的参数，并且简化了模型。而且由于可能会过拟合，参数越少越好。

具体步骤：使用一个BERT作为编码器，以获得支持集S和查询集Q的上下文嵌入，对于S和Q的实例，中间状态是通过连接实体开始标记的隐藏状态而获得的。

对S中每个关系类的中间状态求平均，以获得每个关系类初始原型表示。

对于每个关系，连接名称和描述，并输入到BERT编码器中，处理[CLS]标记的嵌入，以及所有令牌嵌入的平均值，两个作为关系表示不同的两个观点。

关系表示生成

为了最小化的减少多余参数的加入，直接利用关系表示的两个方面拼接得到的
$$
R^{final} = R^{view1}⊕R^{view2}
$$
最终的原型向量表示为原始原型和最终的关系表示相加来
$$
P^{final} = P + R^{final} = \{P^f_i∈R^{2d}\}
$$
模型使用矢量点积方法来计算查询集Q和每个类原型之间的距离

利用交叉熵损失来作为损失函数
$$
L_{cd}=-\log(z_y)
$$
最后和HCPR来进行比较，实验结果相差无几，可以说很简单的方式直接实现了利用很复杂网络或者方法才能达到的效果。



---

HCPR方法

## `Exploring Task Difficulty for Few-Shot Relation Extraction`

发布于EMNLP 2021，提出了HCPR的方法



针对于硬任务，设计了一种任务自适应焦点损失，改进的交叉熵（CE）损失，CE损失可以写成：
$$
L_{CE} = -\log(z_y)
$$
$z_y$是类y的估计概率，y是类别标签，这个焦点的损失的提出指出解决硬例子和简单例子之间的不平衡

TODO：最后的方法但是可能和刚开始定义的原型网络提出的一些思路有些偏离。因为在原论文中提出来的数学方法证明来一些数学方法的可行性。
$$
L_F = -(1-z_y)^y\log(z_y)
$$
y这个系数就是为了调整简单任务和硬粒度任务之间，为了减少细粒度的权重，然后增加硬粒度的权重。

使用焦点损失而不是交叉熵损失来更多地关注硬查询实例，此外，为了更专注于硬任务，设计了一种新的任务自适应焦点丢失，该算法引入了动态任务级别权重。通过估计类间相似性来估计任务的难度。

所有的一切提出来都是为了解决硬任务，为了解决细粒度

具体而言如下：

混合原型学习:

全局原型表示：模型嵌入用的是BERT来进行emmbeding，全局模型由两部分组成，一部分是[cls]的标记的隐藏状态，另一部分就是根据Snell等人的工作对k个支持实例的全局特征进行平均，就是支持集的平均

加入全局变量的目的：为了更好的进行泛化，更好地识别其他的新类。

全局原型表示：分为两个部分，而且都是有BERT来进行嵌入的，第一个是**全局特征**的支持集和查询集来说就是通过连接提到的两个实体的开始标记相对应的隐藏状态而获得的（MTB中提到的方法）。第二部分是**关系的全局特征**通过对应的[CLS]标记的隐藏状态（变换成2d维），对于每个关系i，通过利用原型网络论文中的对k个支持实例的**全局特征**进行平均。然后加上**关系的全局特征**形成全局的原型表示。
$$
p^i_g = {\frac {1}{K}}\sum^K_{k=1}s^i_k + r^i ∈R^{2d}
$$


全局特征的缺点：全局特征可以捕获一般的数据表示，但是不容易捕获特定的RSRE任务中的有用本地信息。为了处理更细粒度的FSRE任务，进一步提出了本地原型，以突出实例中对表征不同关系的重要标记。

局部信息：全局原型能够捕获一般的数据表示，但是这种表示可能不容易捕获特定RSRE任务中的有用本地信息。为了更好地处理高度相似关系之间存在细微差异的硬FSRE任务，进一步提出了本地原型。目的就是为了更好的处理细粒度任务。

局部特征表示：对于关系i，首先计算第k个支持实例的局部特征
$$
\hat s^i_k = \sum ^{l_s{^i_k}}_{n=1}\alpha^s_n[S^i_k]_n∈R^d
$$

$$
\alpha= softmax(sum(S^i_k(R^i)^T))∈R^{l{_s{^i_k}}}
$$

$[.]_n$是矩阵的第n行，sum()是对矩阵每行的所有元素求和运算。具体来说，就是通过根据不同的token(分词)和关系描述的相似性将权重分配给它们，然后加权和来形成这些局部特征。

类似地，计算关系嵌入$R^i$和关系i的每个支持实例嵌入之间相似性，并获得k个特征
$$
\hat r^i_k = \sum^{l{_r{^i}}}_{n=1}\alpha^r_n[R^i]_n∈R^d
$$

$$
\alpha^r = softmax(sum(R^i(S^i_k)^T))∈R^{l{_r{^i}}}
$$

对于k个特征进行平均，以获得关系i的最终局部表示：
$$
\hat r^i = {\frac 1 k}\sum^K_{k=1}\hat r^i_k∈R^d
$$
查询实例的本地特征由下面的公式计算：
$$
\hat q _j = \sum ^{l_{q_j}}_{n=1}\alpha^q_n[Q_j]_n∈R^d
$$

$$
\alpha^q = softmax(sum(Q_jQ^T_j))∈R^{l_{q_{j}}}
$$

最后，通过平均支持集的局部特征加上关系的局部特征来生成局部原型
$$
p^i_l = {\frac 1 k}\sum^K_{k=1}\hat s^i_k + \hat r^i ∈R^d
$$
对于全局关系和局部关系的总结：全局关系为了更好的进行泛化，让模型具有普遍特征，局部信息有让每个类拥有更好的特征，区别于其他的关系类的特征。

混合原型：

拼接原型，把全局原型和局部原型拼接起来
$$
p^i_h = [p^i_g;p^i_l]∈R^{3d}
$$

$$
q^j_h = [q_j;\hat q _j] ∈ R^{3d}
$$

通过表示查询和N个关系的原型，模型计算查询实例$q_j$的关系概率是
$$
z(y=i|q_j)={\frac {exp(q^j_h·p^i_h)} {\sum^N_{n=1}exp(q^j_h·p^n_h)}}
$$


关系原型的对比学习：

硬任务通常涉及原型表示接近的相似关系，从而增加了对查询实例进行分类的挑战。为了更具辨别力的原型表示，提出一个关系原型的对比学习。

提出一个RPCL的方法，该方法利用可解释的关系名称和描述来校准原型，就是利用关系（关系名称和描述作为锚）与传统的无监督和自我监督对比学习不同，PRCL利用每个任务中支持实例的标签来执行监督对比学习。

具体来说，以关系表示为锚，同一类的原型为正，不同类的原型为负，RPCL旨在将正与锚拉近，将负推开，对于将混合表示的特征关系i（TODO：还有一个相关论文，至把关系作为对比学习的正负，CP方法来说就是用不同关系作为对比学习的正负）原型来说就是关系类
$$
r^i_h = [r^i;\hat r ^i]∈R^{3d}
$$


该模型收集了正原型和负原型，目标是区分积极和消极。使用点积来测量关系锚和选定原型之间的相似性。
$$
u^i_{pos} = P^i_h·r^i_h∈R
$$

$$
u^{i,n}_{neg} = P^n_h·r^i_h∈R
$$

对比度损失通过公式计算：
$$
L_C = \sum ^N_{i=1}-\log{\frac {u^i_{pos}} {u^i_{pos}+\sum_nu^{i,n}_{neg}}}
$$
自适应焦点loss：为了更好的分辨硬任务，让模型更多的注意相似类之间的区别

结果;

![image-20221116140433984](C:\Users\jerry\AppData\Roaming\Typora\typora-user-images\image-20221116140433984.png)





---

论文发布于

ACL2019年7月

## `Matching the Blanks: Distributional Similarity for Relation Learning`

首先就是讨论了什么样的关系抽取模型在BERT上效果最好，然后提出了一个关系抽取与训练模型。

对于BERT的有两条考虑：

输入形式：怎么基于BERT的关系抽取模型构造输入(输入形式)

关系表示：如何表示文本中的实体对(关系表示)



3种输入形式：

Standard input: 最一般的输入，不对实体对进行特殊表示

Positional embeddings: 为输入序列中的头实体E1分配ID1，尾实体E2分配ID2，其他token分配ID0，进行embedding后与word embedding 相加

Entity marker tokens: 在实体前后加入特殊token, 即在头实体E1前后分别加上\[E1][\E1]，同理，尾实体E2前后加上\[E2][\E2]

3种关系表示。

[CLS] token: 即直接使用[CLS] token的表示作为关系表示

Entity mention pooling: 将实体进行max-pooling 得到实体表示，再拼接头尾实体得到关系表示

Entity start state: 拼接头尾实体的开始标记。

输入和关系表示可以组合成6种形式，：

![image-20221114172420878](C:\Users\jerry\AppData\Roaming\Typora\typora-user-images\image-20221114172420878.png)

然后在有监督的方法和小样本关系分类中进行试验，使用了同一个loss方法，即关系表示使用点积得到该关系表示为某个关系类别的分数，然后使用softmax得到各个关系类别的概率，loss如图;

![img](https://pic3.zhimg.com/v2-bc70422e4339e2d3828b7d1ecea65fa2_r.jpg)

试验结果：

![img](https://pic3.zhimg.com/v2-0ef2f6b67ab9aa3fd792985548ccf95e_r.jpg)

**试验结果都证明“输入形式：在头尾实体前后加特殊token + 关系表示：拼接头尾实体的开始标记”在4个数据集上效果都是最好的。**后面所有的试验都是在这个结论下进行的

作者提出一个假设：含有相同实体对的句子应该具有相似的关系表示。

根据这个假设提出一个关系抽取预训练任务，称为MTB(Matching the Blanks)。该方法采用的是对比学习，且基于假设。

预训练构造：

数据采集和实体链接：作者使用实体链接工具对英语维基百科的文本进行实体链接标注，且只用实体链接的结果，也就是上文的结论。

正负样本构造：包含相同实体对的句子两两结合形成正样本；负样本有两部分，包含不同实体对的句子和仅含有一个相同实体的两个句子作为负样本。

对实体进行mask：直接使用实体本身训练上述预训练模型，由于同一实体字面量相似度很高，所以loss就很容易下降，但是模型并没有学到语义，所以作者就引入了一个mask机制，即以$\alpha$=0.7的概率来把实体替换成特殊的token[BLANK]



结果：

![img](https://pic4.zhimg.com/v2-f9cd7efa2d5811c686ad064470d34903_r.jpg)



 

论文发布于

EMNLP2021年10月

## `Learning from Context or Names? An Empirical Study on Neural Relation Extraction`

对于上一篇的改进，也是分为两个部分：证明了对于关系抽取来说实体本身和实体对所在的上下文的影响程度；在前者的基础上改进MTB关系抽取预训练方法。

假设：“含有相同关系的句子应该具有相似的关系表示”，和上面的MTB中的相同实体具有相似的关系表示有区别

这里的关系是指在KG里是同一个关系。根据这个假设提出了一个新的预训练模型(Contrastive Pre-training)

![img](https://pic2.zhimg.com/v2-8d375b3a72827369fdc8a0523e8ed059_r.jpg)

CP和MTB最大的区别在于预训练样本对构造方式不同，图右边展示说明，对于一个目标对，MTB采用其对应的正样本即包含和目标相同实体对的句子，负样本为实体对不同的句子。

CP采样其正样本时与目标样本在KG中具有相同关系的句子，负样本是在KG中拥有不同关系的句子。此外CP继续使用BERT预训练(MLM)任务。

整篇论文探讨了一下，上下文和实体名称对关系抽取的哪个作用比较大。

文章用试验来证明：

Context + Mention：保留上下文和实体名称

Context + Type：保留上下文，将实体名称换成类别，其中类别是特殊的一种token

Only Context：保留上下文，头尾实体名称分别替换成[SUBJ]和[OBJ]

Only Mention：丢弃所有的上下文

Only Type：在Only Mention基础上，将实体名称替换成类别。

![img](https://pic2.zhimg.com/v2-ba76b37f0b2c509f91c9c7ece80c5be1_r.jpg)

结论：

1. 只依靠一种信息，是不靠谱的
2. 实体名称提供的主要是类别信息
3. 实体名称可能会提供一些浅显的利于区分关系的线索
4. 当模型无法很好理解上下文时，它们会倾向于依赖实体名称提供的信息

结果：

![img](https://pic1.zhimg.com/v2-f4c67a126fcc0fa46da3cffa45de80d0_r.jpg)

![img](https://pic1.zhimg.com/v2-6b714b766d45642c516712879efc24b4_r.jpg)

结论：

1. 上下文和实体名称的信息都很重要
2. 实体名称会泄露一些利于区分关系的线索，到时模型无法更好地了解上下文
3. 基于以上两点，本篇提出了“针对关系抽取的，实体名称遮蔽的，对比学习，预训练框架”



---

**总结**

现在就要想出来自己要解决什么问题，然后根据这个问题要提出什么方法来解决

上面所有的工作都是为了更好的表示一个原型向量，无论是PRM、HCPR、MTB、CP，然后问题更加细化。

关系表示：头尾实体token标记直接拼接。（论文中一般都是平均K个关系表示）

\[CLS]：可不可以当这个为包含了上下文信息。

上面这几篇论文用的很杂，有的只是用平均了关系表示，有的平均了关系表示之后加上了[CLS]，不知道哪个方式更好



可不可以直接bank关系



数据增强，迁移学习，降噪

可不可以直接bank关系，因为还有外部信息来作为关系输入，然后[CLS]中的语义信息和实体前面平均的拼接信息作为
